# House Price Prediction: Multi-Model Machine Learning Pipeline
## Project Overview
This repository contains a complete, end-to-end machine learning pipeline built for the Kaggle **"House Prices - Advanced Regression Techniques"** competition. The objective of this project is to predict residential home prices in Ames, Iowa, based on 79 explanatory variables. 

The primary focus of this codebase goes beyond basic modeling; it emphasizes strict data science best practices, including **data leakage prevention**, **domain-aware imputation**, and **automated multi-model hyperparameter tuning**.

## Repository Contents
* **`train.csv` & `test.csv`**: The raw datasets provided by Kaggle.
* **`final_script.py`**: The main Python script containing the full machine learning pipeline, from data ingestion to Kaggle submission generation.
* **`Technical_Report.pdf`**: A comprehensive ~1,000-word academic report detailing the methodology, architectural decisions, and visual insights of the champion model.
* **`output_file.csv`**: The final predicted housing prices generated by the Champion Model, formatted for Kaggle API submission.

## Methodology & Pipeline Architecture

### 1. Target Transformation & Leakage Prevention
* **Log Transformation:** The target variable (`SalePrice`) exhibits extreme positive skew. It was transformed using `np.log1p` to align the model's objective function with Kaggle's evaluation metric (Log-RMSE) and prevent the model from over-indexing on luxury mansions.
* **Leakage Prevention:** To ensure complete statistical isolation, the data was partitioned using `train_test_split` *before* any summary statistics were calculated. All preprocessing rules were mathematically fitted exclusively on the training split.

### 2. Domain-Aware Feature Preprocessing
Missing data was handled using a highly customized `ColumnTransformer` engineered to treat features based on their real-world meaning:
* **Mode Imputation:** Applied to discrete numerical features (e.g., `GarageCars`, `BsmtFullBath`).
* **Mean Imputation:** Applied to continuous metrics where average distributions are logically sound (e.g., `LotFrontage`).
* **Median Imputation:** A robust catch-all for remaining numerical features to mitigate the pull of extreme real-estate outliers.
* **Constant Categorical Imputation:** Heavily missing categorical columns (like `PoolQC` or `Fence`) were intentionally retained. Missing values were imputed with the string `'None'` to ensure the model successfully captured the *absence* of these physical features as predictive signals.

### 3. Multi-Model Champion Selection
The pipeline leverages a dictionary-based loop to dynamically test and tune three distinct algorithms via `GridSearchCV` (3-fold cross-validation):
1. **Linear Regression:** Evaluated as a baseline.
2. **Random Forest Regressor:** Tuned specifically for tree depth (`max_depth`: [10, None]) and forest size (`n_estimators`: [100, 300]) to balance its bagging ensemble nature against the heavily expanded One-Hot Encoded feature space.
3. **XGBoost Regressor:** Tuned for a slow, highly regularized gradient boosting approach (`learning_rate`: [0.05, 0.1], shallow `max_depth`: [2, 5]) to prevent overfitting on noisy categorical data.

The script automatically identifies the algorithm with the lowest validation Log-RMSE, crowns it the "Champion Model," plots its predictions against the actual validation curve, and generates the final predictions.

## Future Improvements (Reflective Wrap-up)
Given more time to experiment, the following steps would be taken to further optimize the Log-RMSE score:
* **Advanced Feature Engineering:** Combining basement, 1st floor, and 2nd floor square footage into a single `Total_Square_Footage` metric, and calculating `Age_at_Sale`.
* **Outlier Filtering:** Manually removing the extreme outlier homes (4,000+ sq. ft. sold for abnormally low prices) noted by the original dataset author to prevent skewed decision boundaries.
* **Alternative Algorithms:** Experimenting with **CatBoost** (which handles categorical text natively without One-Hot Encoding) or building a **Stacking Regressor** to combine the predictions of the Random Forest and XGBoost models.

## How to Run
The Python script is designed to be 100% portable. It automatically pulls the `train.csv` and `test.csv` files directly from the raw GitHub URLs, meaning you do not need to download the datasets locally to run the code.

Simply clone this repository, install the dependencies, and run the script:

```bash
git clone [https://github.com/Shehrhass/Kaggle-Competition-ML-Multimodel-Prediction.git](https://github.com/Shehrhass/Kaggle-Competition-ML-Multimodel-Prediction.git)
cd Kaggle-Competition-ML-Multimodel-Prediction
pip install pandas numpy scikit-learn xgboost matplotlib seaborn sweetviz
python final_script.py
